---
title: "Lab 5"
author: "Liam Spoletini"
date: "6/22/2022"
output: pdf_document
---

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(boot))
suppressPackageStartupMessages(library(leaps))
suppressPackageStartupMessages(library(glmnet))
suppressPackageStartupMessages(library(pspline))

auto = read_csv("~/Documents/MSDS/Summer-627/Datasets/Auto.csv", 
                show_col_types = F)
auto %>%
  mutate(horsepower = as.numeric(horsepower)) %>%
  filter(!is.na(horsepower)) -> auto
```

# Part 1 - The Bootstrap
## Question 1
```{r}
B = 1000
# Define IQR function
iqr.fn <- function(X,index){ 
  return(IQR(X[index]))
}
bs.mpg.iqr <- boot(auto$mpg, iqr.fn, R=B)
```
**Bootstrap Estimate:**
```{r}
mean(bs.mpg.iqr$t)
```
**Standard Error:**
```{r}
sd(bs.mpg.iqr$t)
```
**95% Confidence Interval:**
```{r}
alpha <- 0.05
c(quantile(bs.mpg.iqr$t,alpha/2),quantile(bs.mpg.iqr$t,1-alpha/2))
```

## Question 2
### a.
```{r}
ggplot(data=auto) +
  geom_point(mapping= aes(y=mpg, x=horsepower)) +
  geom_smooth(mapping = aes(y=mpg, x=horsepower),
              formula = y ~ x + I(x^2), 
              method = "lm",
              se = FALSE)
```

### b.
```{r}
# 1: Get Bootstrap Coefficients
coefs.fn <- function(dataset, index){
  x = coef(lm(mpg ~ horsepower + I(horsepower^2), data = dataset[index,]))
  return(x)
}
bs.quad <- boot(auto, coefs.fn, R=100)

# Plot object
quad.plot <- ggplot() + 
  geom_point(mapping = aes(y = mpg, x=horsepower), data=auto)

# Get estimates
temp = matrix(nrow=392, ncol=100)
for(i in 1:100){
temp[,i] <- bs.quad$t[i,1] + auto$horsepower*bs.quad$t[i,2] + 
                  (auto$horsepower^2)*bs.quad$t[i,3]
}
temp = as.data.frame(temp)
temp_m = reshape2::melt(temp)
temp_m$horsepower = rep(auto$horsepower, 100)
for(i in 1:100){
  quad.plot <- quad.plot + geom_line(aes(x = horsepower,
                y = value, group=variable), color ="gray", 
                size=0.1, data=temp_m)
}
print(quad.plot)

```

# Part 2 - Regularization, PCR, and Splines
```{r}
n <- 100
sig <- 1
b <- c(1.5,-2,-0.5,1) # beta coefficients 
set.seed(42)
x <- sort(round(runif(n,-2,2),2)) # predictor x
eps <- rnorm(n,0,sig) # random noise
ones <- vector(length=n)+1
x2 <- x^2
x3 <- x^3
xmat <- cbind(ones, x, x2, x3)
head(xmat)
fx <- xmat%*%b # true nonlinear relationship is cubic 
y <- fx + eps
data <- data.frame(x,y) # this is the sample data 
plot(x, fx, type='l',lwd=2, ylim=c(-5,5), main='True relationship between y and x with generated sample')

points(x,y,pch=19)
```

## Question 1
$$
y = 1.5 - 2x - 0.5x^2 + x^3
$$
## Question 2

```{r}
set.seed(42)
x <- sort(round(runif(n,-2,2),2)) # predictor x
data = tibble(X=x)
data$X2 = x^2
data$X3 = x^3
data$X4 = x^4
data$X5 = x^5
data$X6 = x^6
data$X7 = x^7
data$X8 = x^8
data$X9 = x^9
data$X10 = x^10
subset.obj <- regsubsets(y~ ., data, nvmax = 10)

```

### a.
```{r}
plot(subset.obj, scale = "Cp")
```
Best model:

$$
y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^5 + \beta_4x^7
$$
```{r}
coef(subset.obj, 4)
```

### b.
```{r}
plot(subset.obj, scale = "bic")
```
Best model:

$$
y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3
$$
```{r}
coef(subset.obj, 3)
```

### c.
```{r}
plot(subset.obj, scale = "adjr2")
```
Best model:

$$
y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^5 + \beta_4x^7
$$
```{r}
coef(subset.obj, 4)
```

### d.
```{r}
# Generate yhats
yh_cp <- cbind(ones, data$X, data$X2, data$X5, data$X7) %*% coef(subset.obj,4)
yh_bic <- cbind(ones, data$X, data$X2, data$X3) %*% coef(subset.obj,3)

plot(x,y, pch=20)
lines(x, yh_cp,col="blue")
lines(x, yh_bic, col="red")
legend(-0.5,-3,legend=c("Cp and Adj. R^2", "BIC"),
       col= c("blue", "red"),
       lty=1)

```
Cp and Adjusted R^2 selected the same model, so they are both represented by the blue line.  It seems as if the ends of the Cp/Adj. R^2 lines curve, which may not generalize well to new data.  The BIC-selected model, however, has the same polynomial degree as the true function.

## Question 3
```{r}
subset.obj.f <- regsubsets(y~ ., data, nvmax = 10, method="forward")
subset.obj.b <- regsubsets(y~ ., data, nvmax = 10, method="backward")

# To save space, I repeated the plots and got the necessary info from them to get regression coefficients and make the graphs.
# Forward Regression
plot(x, y, pch = 20)
yh_cp_bic <- cbind(ones, data$X, data$X2, data$X3, data$X9) %*% coef(subset.obj.f,4)
yh_adjr2 <- cbind(ones, data$X, data$X2, data$X3, data$X5, data$X9) %*% coef(subset.obj.f,5)
lines(x, yh_cp_bic,col="blue")
lines(x, yh_adjr2, col="red")
legend(-0.5,-3,legend=c("Cp and BIC", "Adj. R^2"),
       col= c("blue", "red"),
       lty=1)
```
```{r}
# Backward Regression
plot(x, y, pch = 20)
yh_all <- cbind(ones, data$X, data$X2, data$X5, data$X7) %*% coef(subset.obj.b,4)
lines(x, yh_cp_bic,col="blue")
legend(-0.5,-3,legend=c("Cp,BIC, and Adj. R^2"),
       col= c("blue"),
       lty=1)
```
The Forward regression identified the model including X, X^2, X^3, and X^9 as the best using Cp and BIC, and added X^5 onto that when using Adjusted R^2. Similar to the results from Question 2, one model seems to have more flexibility at the ends of the x range and is overfit compared to the others.  It does not seem that the X^9 term has a huge impact on the model since the shape of the simpler model looks roughly cubic.

*Forward Regression Coefficients*
```{r}
coef(subset.obj.f, 4)
```
```{r}
coef(subset.obj.f, 5)
```

The Backward regression identified the same variable subset as the Cp/Adj. R^2 best model from Question 2.

```{r}
coef(subset.obj.b, 4)
```

## Question 4
```{r}
xl <- model.matrix(y ~ ., data)[,-1]

bestlam = cv.glmnet(xl,y,alpha=1)$lambda.min

lasso1 <- glmnet(xl, y, alpha=1, lambda = bestlam)
coef(lasso1)
```
The resulting fitted model includes the variables in the table above with the coefficients also shown above in the s0 column.

## Question 5
```{r}
m2 <- sm.spline(x,y,df = 2)
m3 <- sm.spline(x,y,df = 3)
m4 <- sm.spline(x,y,df = 4)
m5 <- sm.spline(x,y,df = 5)
m15 <- sm.spline(x,y,df = 15)
mcv <- sm.spline(x,y,cv=T)
mcv
```
The optimal DF is 7.440599 according to the CV results.

```{r}
plot(x,y, pch=20)
lines(m2, col=1)
lines(m3, col=2)
lines(m4, col=3)
lines(m5, col=4)
lines(m15, col=5)
lines(mcv, col=6)
legtxt <- c("df = 2","df = 3", "df = 4", "df = 5", "df = 15","df CV")
legend("bottomleft",legtxt,lty=c(1,1,1,1,1,1),col=c(1,2,3,4,5, 6),lwd=2)
```
As we've come to expect with splines at this point in the course, they show an increasingly good fit with the data as the degrees of freedom increase up to a certain point. The spline with 15 degrees of freedom has a very flexible shape that would change a lot with noise.  The optimal DF selected by cross-validation is around 7.44, much less than the highest df tested.

## Question 6

Since we're working with simulated data, we can get the test MSE's of the models by simulating new data.

```{r}
# New data
set.seed(50)
x_new = sort(round(runif(300,-2,2),2))
eps_new = rnorm(n,0,sig)

x2_new <- x_new^2
x3_new <- x_new^3
ones_new <- vector(length=300)+1
xmat_new <- cbind(ones_new, x_new, x2_new, x3_new)
fx_new <- xmat_new%*%b # true nonlinear relationship is cubic 
y_new <- fx_new + eps_new
data_new <- data.frame(X=x_new) # this is the sample data 

# Polynomial terms

data_new[,"X2"] = data_new$X^2
data_new[,"X3"] = data_new$X^3
data_new[,"X4"] = data_new$X^4
data_new[,"X5"] = data_new$X^5
data_new[,"X6"] = data_new$X^6
data_new[,"X7"] = data_new$X^7
data_new[,"X8"] = data_new$X^8
data_new[,"X9"] = data_new$X^9
data_new[,"X10"] = data_new$X^10

data_new = data.matrix(data_new)
head(data_new)

```
```{r}
# Getting Test MSEs for each Model

MSE = vector(length=16)
# Normal Stepwise
# 1. Best Cp
coefs = coef(subset.obj, id= 4)
coef_names = names(coefs)[-1]
MSE[1] = sum(((cbind(ones_new, data_new[,coef_names]) %*% coefs) - y_new)^2)

# 2. Best BIC
coefs = coef(subset.obj, id= 3)
coef_names = names(coefs)[-1]
MSE[2] = sum(((cbind(ones_new, data_new[,coef_names]) %*% coefs) - y_new)^2)
# 3. Best Adjusted R^2
MSE[3] = MSE[1]

# Forward Regression
# 4. Best Cp
coefs = coef(subset.obj.f, id= 4)
coef_names = names(coefs)[-1]
MSE[4] = sum(((cbind(ones_new, data_new[,coef_names]) %*% coefs) - y_new)^2)
# 5. Best BIC
MSE[5] = MSE[4]
# 6. Best Adjusted R^2
coefs = coef(subset.obj.f, id= 5)
coef_names = names(coefs)[-1]
MSE[6] = sum(((cbind(ones_new, data_new[,coef_names]) %*% coefs) - y_new)^2)

# Backward Regression
# 7. Best Cp
coefs = coef(subset.obj.b, id= 4)
coef_names = names(coefs)[-1]
MSE[7] = sum(((cbind(ones_new, data_new[,coef_names]) %*% coefs) - y_new)^2)
# 8. Best BIC
MSE[8] = MSE[7]
# 9. Best Adjusted R^2
MSE[9] = MSE[7]

# 10. Lasso

MSE[10] = sum((predict(lasso1, data_new) - y_new)^2)
# Splines
# 11. df = 2
MSE[11] = sum((predict(m2, data_new[,"X"]) - y_new)^2)
# 12. df = 3
MSE[12] = sum((predict(m3, data_new[,"X"]) - y_new)^2)
# 13. df = 4
MSE[13] = sum((predict(m4, data_new[,"X"]) - y_new)^2)
# 14. df = 5
MSE[14] = sum((predict(m5, data_new[,"X"]) - y_new)^2)
# 15. df = 15
MSE[15] = sum((predict(m15, data_new[,"X"]) - y_new)^2)
# 16. CV-selected DF
MSE[16] = sum((predict(mcv, data_new[,"X"]) - y_new)^2)

model_names = c("Best Cp - Stepwise",
            "Best BIC - Stepwise",
            "Best Adj. R^2 - Stepwise",
            "Best Cp - Forward",
            "Best BIC - Forward",
            "Best Adj. R^2 - Forward",
            "Best Cp - Backward",
            "Best BIC - Backward",
            "Best Adj. R^2 - Backward",
            "Lasso",
            "Spline df=2",
            "Spline df=3",
            "Spline df=4",
            "Spline df=5",
            "Spline df=15",
            "Spline CV-selected df"
            )

tibble(model_names, MSE)
```
These models have pretty similar mean squared errors on new data.  However, the best performance was with the Lasso and the only stepwise-selection model that picked X, X^2 and X3.  The latter makes sense because the true relationship is cubic.  The Lasso model also makes sense because it includes the terms in the true regression model, and the coefficients of the other terms it includes are shrunken towards zero.  

As expected, the splines with very low and very high degrees of freedom performed poorly compared to other models.  The spline with the CV-selected degrees of freedom (and the spline with the degrees of freedom closest to optimal) performed on par or better than the stepwise-selection models.

## Question 7

I think that you could reasonably use principal components regression or partial least squares to build a predictive model.  However, I think that the fact that principal components are linear combinations of the original predictors could obscure any inferences you would try to make.  The first principal component could be a linear combination of the first 3 variables, with very small coefficients for the rest of the components. Transforming the features into this space reduces the explainability of the model.  


