---
title: "Final Project"
author: "Liam Spoletini"
date: "6/8/2022"
output: pdf_document
---
# Part 1: Regression

## Overview
Dataset Name: **Wine Quality**

Response Variable: **Quality** (Scale of 1-10, actual range of 3-8)

Predictor Variables:
1. fixed acidity       
2. volatile acidity    
3. citric acid         
4. residual sugar      
5. chlorides           
6. free sulfur dioxide 
7. total sulfur dioxide  
8. density             
9. pH                  
10. sulphates           
11. alcohol             

Number of Instances: **1599**
Number of Missing Values: **0**




## Setup and Exploratory Data Analysis
### Setup
```{r}
source(here::here("R", "Setup.R")) # See Setup.R for details on packages and preprocessing
```
### Train/Test Split
```{r}
n = nrow(reds)
set.seed(42)
Z <- sample(n,n/10)
reds_test = reds[Z,]
reds_train = reds[-Z,]
```
### Graphs

```{r warning=FALSE}
ggplot(reds_train) + geom_histogram(aes(x = quality_numeric), stat="count") + ggtitle("Quality Distribution")
```
The response variable has 6 distinct values that range from 3 to 8 (the full dataset and the training data set have this in common.)  This data is ordinal in nature, but there are many training instances available, so we may get good performance by using considering it to be continuous data.
```{r}
reds_long <- reds_train[,1:11] %>%
  pivot_longer(colnames(reds_train)[1:12]) %>% as.data.frame()
ggplot(reds_long) + 
  geom_histogram(aes(x = value)) + 
  facet_wrap( ~ name, scales = "free") + 
  ggtitle("Predictor Distributions")
```
Few predictors are close to normally distributed (pH and density), and all the variables are on very different scales. 

```{r}
res <- cor(cbind(reds_train[,1:12]))
corrplot::corrplot(res, method = "shade", type="lower", tl.col = "black")
```
The correlation plot shows some potential multicollinearity, as fixed acidity is correlated with citric acid and fixed acidity.

**Aproach**
Since the response variable is an ordinal variable, it may make sense to choose a technique that takes this into account.
## Approach #2: Treating the Data as Continuous
### Machine Learning Method: **Principal Components Regression** and **Lasso Regression**

I've decided to use Principal Components Regression because of the potential multicollinearity effects.  I will be comparing the results of PCR to Lasso regression, because later results indicate the data may not be well-suited to PCR.

### Formal Representation of Model

Principal Components Regression consists of two parts: transforming the p predictors into a new feature space, and then picking M of these transformations to use as inputs to a linear regression model.

The first step, called Principal Components Analysis or PCA, transforms the features into p linear combinations of the original features.  These principal components are ordered based on how much of the original variation in the data is captured.  All of the principal components together capture 100% of the original data, but it is sometimes the case that very few (M << p) principal components capture a large portion of the original variation.  Having a lower-dimensional representation of the data that captures much of the variation can be useful because it is resistant to overfitting.

### Assumptions + Considerations

**PCR**

1. Assumption: The directions in which the predictors show the most variation are the directions that are associated with the response, *quality*. (If this holds, then PCR will have a definitive advantage over regular Least Squares regression because it will mitigate overfitting.)

2. Consideration: Predictor variables should be standardized prior to regression because of the tendency for high-variance predictors to dominate the principal components.


### Tuning Parameters

The tuning parameter in PCR is the **number of Principal Components**. A higher number of principal components reduces bias but increases variance.  The tuning parameter for the lasso is **lambda**, which controls how much regularization occurs.

## Implementation and tuning: PCR

```{r}
pc <- princomp(reds_train[,1:11],cor=T)
plot(pc)
```

The first few principal components do not capture much of the variance in the predictors. This may indicate that PCR is not well-suited for this data.

### Training and Tuning
```{r}
model_PCR = pcr(quality ~ .,
  data = reds_train, 
  scale = T,
  center = T)

# Tuning method: 10-fold Cross-Validation
validationplot(model_PCR, val.type = "MSEP")
```
The results here do not indicate that the data are well-suited for Principal Components Regression.  There is not a clear minimum in the validation plot, and the lowest CV score was found with the maximum number of components M that is equal to the number of original predictors, 11. Since many principal components are required, it might be a good idea to look at other methods.  It is possible that not all the original predictors are strictly necessary to predict the response, and a subset-selection method like the Lasso could perform better.  When testing on the holdout set, I will use a model with 5 principal components.  This is because the 10-fold cross-validation error is similar when using a PC number of 3-10, and using fewer principal components is better for preventing overfitting.

## Lasso

```{r}
x <- model.matrix(quality ~ ., reds_train)[, -1]
grid <- 10^seq(1,-3,length=300)
set.seed(42)
cv.lasso.1 <- cv.glmnet(scale(x), reds_train$quality, nfolds = 10, alpha = 1, lambda = grid)
cv.lasso.1$lambda.min
```

```{r}
plot(cv.lasso.1)
```
There is a lower MSE for lower values of Lambda.  Lambda values less than 0.0001 all give relatively similar 10-fold CV error, so I will use 0.0001 to build the model.  Small Lambda values giving small errors seems to indicate that less regularization is better than more and, in this case, that all predictors are relevant.

```{r}
lasso.mod <- glmnet(scale(x), reds_train$quality, alpha = 1, lambda = cv.lasso.1$lambda.min)
coef(lasso.mod)
```
The Lasso did not remove any variables from the model.  This indicates that all the variables are important to the model.

## Applying to Test Data

```{r}
model.PCR.final <- pcr(quality ~ ., data = reds_train, ncomp=5, scale = T, center = T)
lasso.mod <- glmnet(scale(x), reds_train$quality, alpha = 1, lambda = cv.lasso.1$lambda.min)

Yhat_PCR <- predict(model.PCR.final, newdata = reds_test[,1:11])
Yhat_lasso <- predict(lasso.mod, cv.lasso.1$lambda.min, newx=scale(reds_test[,1:11]) )
```

RMSEs:  
```{r}
sqrt(mean((Yhat_PCR - reds_test$quality)^2))
sqrt(mean((Yhat_lasso - reds_test$quality)^2))
```

Principal Components regression performed poorly compared to lasso regression.  This is likely because the problem was poorly suited to principal components regression.  Most of the variation in the data was *not* contained in the first few principal components.  The Lasso regression did not identify any variables that could be dropped from the model.  


# Classification

## Overview

### Dataset  
Name: **Heart Disease Data set**  

Response Variable:  
**Angiographic disease status**:  
0: <50% diameter narrowing  
1: >50% diameter narrowing  

Predictor Variables (13 total):  
Categorical:  
1. Sex  
0 = female  
1 = male  

2. Chest Pain Type (cp)  
1 = typical angina 
2 = atypical angina 
3 = non-anginal pain 
4 = asymptomatic 

3. Fasting Blood Sugar (fbs)  
0 = fbs < 120 mg/dl
1 = fbs > 120 mg/dl

4. Resting Electrocardiographic results (restecg)  
0 = normal  
1 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)   
2 = showing probable or definite left ventricular hypertrophy by Estes' criteria   

5. Exercise-Induced Angina (exang)  
0 = no  
1 = yes  

6. The slope of the peak exercise ST segment  (slope)  
1 = upsloping   
2 = flat   
3 = downsloping   

8. thal  
3 = normal  
6 = fixed defect  
7 = reversible defect   

Quantitative

1. Age  
2. resting blood pressure (in mm Hg on admission to the hospital) (trestbps)  
3. serum cholestoral in mg/dl (chol)  
4. maximum heart rate achieved (thalach)  
5. ST depression induced by exercise relative to rest (oldpeak)  
6. Number of major vessels (0-3) colored by flourosopy (ca)  

## Missing Data
```{r warning=FALSE}

heart_data %>% 
  mutate(ca = as.numeric(ca), thal = as.numeric(thal)) %>% 
  filter(!is.na(ca), !is.na(thal)) -> heart_data
dim(heart_data)


```
There are 297 instances without NA's.


### Train/Test Split

```{r}
n = nrow(heart_data)
set.seed(42)
Z <- sample(n,n/10)
heart_test = heart_data[Z,]
heart_train = heart_data[-Z,]
```

## Exploratory Data Analysis

```{r}
ggplot(heart_train) + 
  geom_point(aes(x= thalach, y = chol, color=factor(Number)))
```
```{r}
ggplot(heart_train) + 
  geom_point(aes(x= trestbps, y = chol, color=factor(Number)))
```

```{r}
ggplot(heart_train) + 
  geom_point(aes(x= thalach, y = trestbps, color=factor(Number)))
```
The data do not seem to be easily separable when looking at a low number of predictor variables.  Above are a subset of three such combinations of quantitative variables. It is possible that the data is still closer to linearly separable using more of the predictor variables at a time.  

## Machine Learning Methods: **Support Vector Machine**

### Formal Representation of Method:

Support Vector Machine classifiers rely on the idea of a hyperplane that divides the p-dimensional feature space.  This hyperplane is chosen to maximize the margin (the distance between the hyperplane and the training instances in the feature space).  In the case of Support Vector Machines, this hyperplane contains soft margins, and the cost function to be minimzed is a sum of all the violations of the soft margins.  The maximum sum of the cost function itself is a value that can be tuned to produce stricter or more lenient boundaries. 

### Assumptions/Considerations

The textbook doesn't mention any assumptions made about the data that are necessary for SVM.

## Tuning Parameter

The tuning parameter for Support Vector Machines is C, which is the maximum allowed sum of violations to the margin.  A lower value of C allows for fewer violations, and a higher value of C allows for more violations.  The type of kernel is another way we can tune the model.  If we transform the original data using a kernel, the SVM can create a boundary that is linear in the new feature space but nonlinear in the original feature space.  

## Implementation: SVM

Finding the optimal kernel and C combination:

Using 10-fold Cross-Validation on the training set
```{r}
set.seed(42)
rg <- list(kernel=c("linear","polynomial","radial","sigmoid"), cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100))
svmt <- tune(svm, factor(Number) ~ .,data=heart_train, ranges = rg)
svmt
```

The optimal C in this grid was 10, with a linear kernel.

```{r}
rg2 <- list(kernel=c("linear","polynomial","radial","sigmoid"), cost=seq(5, 15, 0.5))
svmt <- tune(svm, factor(Number) ~ .,data=heart_train, ranges = rg2)
svmt
```

The finer grid identified an optimal C of11 paired with a sigmoidal kernel.


## Testing on the Holdout Set

```{r}
svm.tuned <- svm(factor(Number) ~ ., kernel = "sigmoid", cost = 11, data=heart_train)
yhat = predict(svm.tuned, newdata = heart_test[,1:13])
class <- data.frame(yhat = yhat,y = heart_test$Number)
table(class)
```

```{r}
pred <- prediction(as.numeric(yhat==1),as.numeric(heart_test$Number==1))
perf <- performance(pred,"auc")@y.values
perf
```

Summary Statistics:
Total error rate = 17.2%
AUC: 0.844

Through both the total error rate and the Area Under the Curve score, we can see that the SVM classifier is doing an okay job correctly identifying test set patients' heart disease status.  However, there are more false negatives in the dataset than false positives, and this is not the kind of classifier you want in a disease-detection setting. Below I will compare the results to a simple logistic regression model.

```{r}
log.model <- glm(factor(Number) ~ ., data=heart_train, family = "binomial")
yhat = predict(log.model, newdata= heart_test[,1:13], type = "response")
class <- data.frame(yhat = as.numeric(yhat > 0.5),y = heart_test$Number)
table(class)
```

The logistic regression model has exactly the same distribution of predictions as the SVM model, which makes sense given the explanation of the two methods' relationship to each other detailed in the textbook.